{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd0b4c0d",
   "metadata": {},
   "source": [
    "## Setup: Import Observability Modules\n",
    "\n",
    "We'll use the modules we created in `src/observability/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db384d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Observability modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# We import Prometheus first so we can clear old collectors before\n",
    "# observability.metrics registers new ones (prevents duplicate errors)\n",
    "from prometheus_client import REGISTRY\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Clear any existing metrics from previous runs (helpful when rerunning cells)\n",
    "collector_map = getattr(REGISTRY, '_collector_to_names', None)\n",
    "if collector_map:\n",
    "    for collector in list(collector_map.keys()):\n",
    "        try:\n",
    "            REGISTRY.unregister(collector)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "# Import our observability modules (safe after registry cleanup)\n",
    "from observability.tracer import ContractAgentTracer\n",
    "from observability.metrics import MetricsCollector, timed_operation\n",
    "from observability.logger import ContractAgentLogger\n",
    "\n",
    "\n",
    "print(\"‚úÖ Observability modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bdc4d0",
   "metadata": {},
   "source": [
    "## Part 1: Structured Logging\n",
    "\n",
    "Structured logging outputs JSON instead of plain text, making it:\n",
    "- **Searchable** - Query by fields\n",
    "- **Parseable** - Automated analysis\n",
    "- **Contextual** - Rich metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "348fe607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 11:14:38,773 - notebook_demo - INFO - Contract analysis started\n",
      "2025-11-29 11:14:38,774 - notebook_demo - INFO - LLM call completed\n",
      "2025-11-29 11:14:38,775 - notebook_demo - ERROR - Classification failed\n",
      "\n",
      "‚úÖ Logged 3 structured events\n",
      "\n",
      "Note: In production, these would go to a log aggregation system like:\n",
      "  ‚Ä¢ Elasticsearch + Kibana\n",
      "  ‚Ä¢ Splunk\n",
      "  ‚Ä¢ Datadog\n",
      "  ‚Ä¢ CloudWatch Logs\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure src is on the path if setup cell was skipped\n",
    "src_path = Path.cwd().parent / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import logger if it is missing from the namespace\n",
    "try:\n",
    "    ContractAgentLogger\n",
    "except NameError:\n",
    "    from observability.logger import ContractAgentLogger\n",
    "\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize logger\n",
    "logger = ContractAgentLogger(\"notebook_demo\")\n",
    "\n",
    "# Example 1: Basic logging\n",
    "logger.info(\"Contract analysis started\", extra={\n",
    "    \"request_id\": \"req-12345\",\n",
    "    \"user_id\": \"user-001\",\n",
    "    \"contract_type\": \"NDA\"\n",
    "})\n",
    "\n",
    "# Example 2: Performance logging\n",
    "logger.info(\"LLM call completed\", extra={\n",
    "    \"duration_ms\": 1234,\n",
    "    \"tokens_used\": 450,\n",
    "    \"model\": \"gpt-4o-mini\"\n",
    "})\n",
    "\n",
    "# Example 3: Error logging\n",
    "logger.error(\"Classification failed\", extra={\n",
    "    \"error_type\": \"ValidationError\",\n",
    "    \"error_message\": \"Invalid contract format\"\n",
    "})\n",
    "\n",
    "print(\"\\n‚úÖ Logged 3 structured events\")\n",
    "print(\"\\nNote: In production, these would go to a log aggregation system like:\")\n",
    "print(\"  ‚Ä¢ Elasticsearch + Kibana\")\n",
    "print(\"  ‚Ä¢ Splunk\")\n",
    "print(\"  ‚Ä¢ Datadog\")\n",
    "print(\"  ‚Ä¢ CloudWatch Logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb93d5d",
   "metadata": {},
   "source": [
    "### Log Context Management\n",
    "\n",
    "Set request-wide context that automatically appears in all logs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e087358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-29 11:14:53,206 - notebook_demo - INFO - Step 1: PDF extraction\n",
      "2025-11-29 11:14:53,207 - notebook_demo - INFO - Step 2: Classification\n",
      "2025-11-29 11:14:53,207 - notebook_demo - INFO - Step 3: Analysis\n",
      "‚úÖ Context automatically added to all log entries\n",
      "‚úÖ Context cleared\n"
     ]
    }
   ],
   "source": [
    "# Set context for the entire request\n",
    "logger.set_request_context(\n",
    "    request_id=\"req-demo-001\",\n",
    "    user_id=\"demo_user\",\n",
    "    session_id=\"session-abc123\"\n",
    ")\n",
    "\n",
    "# All subsequent logs include this context automatically\n",
    "logger.info(\"Step 1: PDF extraction\")\n",
    "logger.info(\"Step 2: Classification\")\n",
    "logger.info(\"Step 3: Analysis\")\n",
    "\n",
    "print(\"‚úÖ Context automatically added to all log entries\")\n",
    "\n",
    "# Clear context when done\n",
    "logger.clear_request_context()\n",
    "print(\"‚úÖ Context cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e90b9b",
   "metadata": {},
   "source": [
    "## Part 2: Prometheus Metrics\n",
    "\n",
    "Metrics track **what's happening** in your system over time.\n",
    "\n",
    "### **Metric Types:**\n",
    "1. **Counter** - Things that only go up (requests, errors)\n",
    "2. **Gauge** - Values that go up/down (queue size, active users)\n",
    "3. **Histogram** - Distribution of values (latency, size)\n",
    "4. **Summary** - Like histogram with percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b7ff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating contract analysis metrics...\n",
      "\n",
      "‚úÖ Recorded NDA analysis (1.2s, 350 tokens)\n",
      "‚úÖ Recorded SaaS analysis (3.5s, 1200 tokens)\n",
      "‚ùå Recorded failed analysis (0.5s)\n",
      "\n",
      "üîí Recorded PII detections (2 emails, 1 phone)\n",
      "\n",
      "‚úÖ Recorded compliance checks\n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics collector\n",
    "metrics = MetricsCollector()\n",
    "\n",
    "# Simulate some contract analyses\n",
    "print(\"Simulating contract analysis metrics...\\n\")\n",
    "\n",
    "# Analysis 1: Successful NDA\n",
    "metrics.record_request(contract_type=\"NDA\", status=\"success\")\n",
    "metrics.record_llm_tokens(model=\"gpt-4o-mini\", operation=\"classify\", tokens=350)\n",
    "metrics.record_duration(operation=\"classify\", duration=1.2)\n",
    "print(\"‚úÖ Recorded NDA analysis (1.2s, 350 tokens)\")\n",
    "\n",
    "# Analysis 2: Successful SaaS\n",
    "metrics.record_request(contract_type=\"SaaS\", status=\"success\")\n",
    "metrics.record_llm_tokens(model=\"gpt-4o-mini\", operation=\"analyze\", tokens=1200)\n",
    "metrics.record_duration(operation=\"analyze\", duration=3.5)\n",
    "print(\"‚úÖ Recorded SaaS analysis (3.5s, 1200 tokens)\")\n",
    "\n",
    "# Analysis 3: Failed (error)\n",
    "metrics.record_request(contract_type=\"Unknown\", status=\"error\")\n",
    "metrics.record_duration(operation=\"classify\", duration=0.5)\n",
    "print(\"‚ùå Recorded failed analysis (0.5s)\")\n",
    "\n",
    "# Security events\n",
    "metrics.record_pii_detection(entity_type=\"email\", count=2)\n",
    "metrics.record_pii_detection(entity_type=\"phone\", count=1)\n",
    "print(\"\\nüîí Recorded PII detections (2 emails, 1 phone)\")\n",
    "\n",
    "# Compliance checks\n",
    "metrics.record_compliance_check(check_type=\"gdpr\", result=\"pass\")\n",
    "metrics.record_compliance_check(check_type=\"data_retention\", result=\"pass\")\n",
    "print(\"\\n‚úÖ Recorded compliance checks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133c24c",
   "metadata": {},
   "source": [
    "### View Current Metrics\n",
    "\n",
    "Prometheus exposes metrics in a specific text format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae92238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Prometheus Metrics:\n",
      "\n",
      "================================================================================\n",
      "contract_analysis_requests_total{contract_type=\"NDA\",status=\"success\"} 1.0\n",
      "contract_analysis_requests_total{contract_type=\"SaaS\",status=\"success\"} 1.0\n",
      "contract_analysis_requests_total{contract_type=\"Unknown\",status=\"error\"} 1.0\n",
      "contract_analysis_requests_created{contract_type=\"NDA\",status=\"success\"} 1.7643951055561466e+09\n",
      "contract_analysis_requests_created{contract_type=\"SaaS\",status=\"success\"} 1.7643951055561466e+09\n",
      "contract_analysis_requests_created{contract_type=\"Unknown\",status=\"error\"} 1.7643951055561466e+09\n",
      "contract_analysis_duration_seconds_bucket{complexity=\"unknown\",contract_type=\"unknown\",le=\"0.1\"} 0.0\n",
      "contract_analysis_duration_seconds_bucket{complexity=\"unknown\",contract_type=\"unknown\",le=\"0.5\"} 1.0\n",
      "contract_analysis_duration_seconds_bucket{complexity=\"unknown\",contract_type=\"unknown\",le=\"1.0\"} 1.0\n",
      "contract_analysis_duration_seconds_bucket{complexity=\"unknown\",contract_type=\"unknown\",le=\"2.0\"} 2.0\n",
      "contract_analysis_duration_seconds_bucket{complexity=\"unknown\",contract_type=\"unknown\",le=\"5.0\"} 3.0\n",
      "contract_analysis_duration_seconds_bucket{complexity=\"unknown\",contract_type=\"unknown\",le=\"10.0\"} 3.0\n",
      "contract_analysis_duration_seconds_bucket{complexity=\"unknown\",contract_type=\"unknown\",le=\"30.0\"} 3.0\n",
      "contract_analysis_duration_seconds_bucket{complexity=\"unknown\",contract_type=\"unknown\",le=\"60.0\"} 3.0\n",
      "contract_analysis_duration_seconds_bucket{complexity=\"unknown\",contract_type=\"unknown\",le=\"+Inf\"} 3.0\n",
      "contract_analysis_duration_seconds_count{complexity=\"unknown\",contract_type=\"unknown\"} 3.0\n",
      "contract_analysis_duration_seconds_sum{complexity=\"unknown\",contract_type=\"unknown\"} 5.2\n",
      "contract_analysis_duration_seconds_created{complexity=\"unknown\",contract_type=\"unknown\"} 1.7643951055561466e+09\n",
      "llm_token_usage_total{model=\"gpt-4o-mini\",operation=\"classify\"} 350.0\n",
      "llm_token_usage_total{model=\"gpt-4o-mini\",operation=\"analyze\"} 1200.0\n",
      "llm_token_usage_created{model=\"gpt-4o-mini\",operation=\"classify\"} 1.7643951055561466e+09\n",
      "llm_token_usage_created{model=\"gpt-4o-mini\",operation=\"analyze\"} 1.7643951055561466e+09\n",
      "pii_detections_total{entity_type=\"email\"} 2.0\n",
      "pii_detections_total{entity_type=\"phone\"} 1.0\n",
      "pii_detections_created{entity_type=\"email\"} 1.7643951055561466e+09\n",
      "pii_detections_created{entity_type=\"phone\"} 1.7643951055561466e+09\n",
      "contract_analysis_active_requests 0.0\n",
      "\n",
      "================================================================================\n",
      "\n",
      "These metrics would be scraped by Prometheus every 15 seconds\n",
      "and visualized in Grafana dashboards.\n"
     ]
    }
   ],
   "source": [
    "from prometheus_client import generate_latest, REGISTRY\n",
    "\n",
    "# Get current metrics in Prometheus format\n",
    "metrics_output = generate_latest(REGISTRY).decode('utf-8')\n",
    "\n",
    "# Show sample metrics\n",
    "print(\"Sample Prometheus Metrics:\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Filter to show our custom metrics\n",
    "for line in metrics_output.split('\\n'):\n",
    "    if 'contract_' in line or 'llm_' in line or 'pii_' in line:\n",
    "        if not line.startswith('#'):\n",
    "            print(line)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nThese metrics would be scraped by Prometheus every 15 seconds\")\n",
    "print(\"and visualized in Grafana dashboards.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5462d86",
   "metadata": {},
   "source": [
    "### Timed Operations Decorator\n",
    "\n",
    "Automatically measure function execution time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ba0944d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running timed operations...\n",
      "\n",
      "‚úÖ PDF extracted: Extracted text\n",
      "‚úÖ Classified as: NDA\n",
      "\n",
      "‚è±Ô∏è  Duration metrics automatically recorded!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "@timed_operation(\"pdf_extraction\")\n",
    "def extract_pdf_simulation():\n",
    "    \"\"\"Simulate PDF extraction.\"\"\"\n",
    "    time.sleep(0.5)  # Simulate work\n",
    "    return \"Extracted text\"\n",
    "\n",
    "@timed_operation(\"contract_classification\")\n",
    "def classify_simulation():\n",
    "    \"\"\"Simulate classification.\"\"\"\n",
    "    time.sleep(0.8)  # Simulate LLM call\n",
    "    return \"NDA\"\n",
    "\n",
    "# Run timed operations\n",
    "print(\"Running timed operations...\\n\")\n",
    "\n",
    "result1 = extract_pdf_simulation()\n",
    "print(f\"‚úÖ PDF extracted: {result1}\")\n",
    "\n",
    "result2 = classify_simulation()\n",
    "print(f\"‚úÖ Classified as: {result2}\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  Duration metrics automatically recorded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5015ed81",
   "metadata": {},
   "source": [
    "## Part 3: Distributed Tracing with OpenTelemetry\n",
    "\n",
    "Traces show the **journey** of a request through your system.\n",
    "\n",
    "### **Trace Anatomy:**\n",
    "```\n",
    "Trace (entire request)\n",
    "‚îú‚îÄ‚îÄ Span: HTTP Request (parent)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Span: PDF Extraction\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Span: Classification\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Span: LLM Call\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ Span: Analysis\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Span: LLM Call\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ Span: Report Generation\n",
    "‚îî‚îÄ‚îÄ Total: 5.2 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1dd1195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenTelemetry tracer initialized\n",
      "   Service: contract-analysis-notebook\n",
      "   Exporting to: http://localhost:4318 (OTLP HTTP)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tracer\n",
    "tracer = ContractAgentTracer(service_name=\"contract-analysis-notebook\")\n",
    "\n",
    "print(\"‚úÖ OpenTelemetry tracer initialized\")\n",
    "print(f\"   Service: contract-analysis-notebook\")\n",
    "print(f\"   Exporting to: http://localhost:4318 (OTLP HTTP)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849fc1c",
   "metadata": {},
   "source": [
    "### Create Traced Operations\n",
    "\n",
    "Use the `trace_span` context manager to instrument your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619de99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Starting traced contract analysis...\n",
      "\n",
      "  ‚úÖ PDF extraction (300ms)\n",
      "  ‚úÖ Security validation (200ms)\n",
      "  ‚úÖ Classification (800ms, 400 tokens)\n",
      "  ‚úÖ Detailed analysis (1500ms, 1500 tokens)\n",
      "  ‚úÖ Compliance check (400ms)\n",
      "  ‚úÖ Report generation (200ms)\n",
      "\n",
      "‚úÖ Contract analysis complete!\n",
      "\n",
      "üìä Trace exported to OpenTelemetry Collector\n",
      "   View in: Jaeger UI (http://localhost:16686) when Docker stack is running\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def simulate_contract_analysis_with_tracing():\n",
    "    \"\"\"Simulate a full contract analysis with distributed tracing.\"\"\"\n",
    "    \n",
    "    # Parent span for the entire operation\n",
    "    with tracer.trace_span(\n",
    "        \"contract_analysis\",\n",
    "        attributes={\n",
    "            \"request.id\": \"req-trace-001\",\n",
    "            \"user.id\": \"demo_user\",\n",
    "            \"contract.type\": \"NDA\"\n",
    "        }\n",
    "    ):\n",
    "        print(\"üîç Starting traced contract analysis...\\n\")\n",
    "        \n",
    "        # Step 1: PDF Extraction\n",
    "        with tracer.trace_span(\n",
    "            \"pdf_extraction\",\n",
    "            attributes={\"file.size\": 52000, \"file.pages\": 3}\n",
    "        ):\n",
    "            time.sleep(0.3)\n",
    "            print(\"  ‚úÖ PDF extraction (300ms)\")\n",
    "        \n",
    "        # Step 2: Security Validation\n",
    "        with tracer.trace_span(\n",
    "            \"security_validation\",\n",
    "            attributes={\"validator\": \"presidio\"}\n",
    "        ):\n",
    "            time.sleep(0.2)\n",
    "            print(\"  ‚úÖ Security validation (200ms)\")\n",
    "        \n",
    "        # Step 3: Classification\n",
    "        with tracer.trace_span(\n",
    "            \"classification\",\n",
    "            attributes={\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"tokens.input\": 350,\n",
    "                \"tokens.output\": 50\n",
    "            }\n",
    "        ):\n",
    "            time.sleep(0.8)\n",
    "            print(\"  ‚úÖ Classification (800ms, 400 tokens)\")\n",
    "        \n",
    "        # Step 4: Detailed Analysis\n",
    "        with tracer.trace_span(\n",
    "            \"detailed_analysis\",\n",
    "            attributes={\n",
    "                \"model\": \"gpt-4o-mini\",\n",
    "                \"tokens.input\": 1200,\n",
    "                \"tokens.output\": 300\n",
    "            }\n",
    "        ):\n",
    "            time.sleep(1.5)\n",
    "            print(\"  ‚úÖ Detailed analysis (1500ms, 1500 tokens)\")\n",
    "        \n",
    "        # Step 5: Compliance Check\n",
    "        with tracer.trace_span(\n",
    "            \"compliance_check\",\n",
    "            attributes={\"checks\": [\"gdpr\", \"data_retention\"]}\n",
    "        ):\n",
    "            time.sleep(0.4)\n",
    "            print(\"  ‚úÖ Compliance check (400ms)\")\n",
    "        \n",
    "        # Step 6: Report Generation\n",
    "        with tracer.trace_span(\n",
    "            \"report_generation\",\n",
    "            attributes={\"format\": \"json\"}\n",
    "        ):\n",
    "            time.sleep(0.2)\n",
    "            print(\"  ‚úÖ Report generation (200ms)\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Contract analysis complete!\")\n",
    "\n",
    "# Run the traced operation\n",
    "simulate_contract_analysis_with_tracing()\n",
    "\n",
    "print(\"\\nüìä Trace exported to OpenTelemetry Collector\")\n",
    "print(\"   View in: Jaeger UI (http://localhost:16686) when Docker stack is running\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b4ef52",
   "metadata": {},
   "source": [
    "### Trace with Custom Attributes\n",
    "\n",
    "Add business context to your traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea638c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running analyses with rich tracing...\n",
      "\n",
      "‚úÖ NDA analyzed\n",
      "‚úÖ SaaS analyzed\n",
      "\n",
      "üìä Traces include business context and events!\n"
     ]
    }
   ],
   "source": [
    "def analyze_with_rich_tracing(contract_type: str, has_pii: bool, complexity: str):\n",
    "    \"\"\"Analysis with rich trace attributes.\"\"\"\n",
    "    \n",
    "    with tracer.trace_span(\n",
    "        \"contract_analysis\",\n",
    "        attributes={\n",
    "            # Business context\n",
    "            \"contract.type\": contract_type,\n",
    "            \"contract.complexity\": complexity,\n",
    "            \"contract.has_pii\": has_pii,\n",
    "            \n",
    "            # Technical context\n",
    "            \"service.version\": \"1.0.0\",\n",
    "            \"environment\": \"production\",\n",
    "            \n",
    "            # User context\n",
    "            \"user.id\": \"user-123\",\n",
    "            \"user.org\": \"ACME Corp\"\n",
    "        }\n",
    "    ) as span:\n",
    "        time.sleep(0.5)\n",
    "        \n",
    "        # Add events to the span\n",
    "        span.add_event(\"Classification completed\", {\n",
    "            \"confidence\": 0.95,\n",
    "            \"contract_type\": contract_type\n",
    "        })\n",
    "        \n",
    "        time.sleep(0.3)\n",
    "        \n",
    "        span.add_event(\"PII detected\", {\n",
    "            \"entity_types\": [\"email\", \"phone\"],\n",
    "            \"count\": 3\n",
    "        })\n",
    "        \n",
    "        return {\"status\": \"success\", \"type\": contract_type}\n",
    "\n",
    "# Run with different contract types\n",
    "print(\"Running analyses with rich tracing...\\n\")\n",
    "\n",
    "result1 = analyze_with_rich_tracing(\"NDA\", has_pii=True, complexity=\"Simple\")\n",
    "print(f\"‚úÖ {result1['type']} analyzed\")\n",
    "\n",
    "result2 = analyze_with_rich_tracing(\"SaaS\", has_pii=False, complexity=\"Complex\")\n",
    "print(f\"‚úÖ {result2['type']} analyzed\")\n",
    "\n",
    "print(\"\\nüìä Traces include business context and events!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1ded7c",
   "metadata": {},
   "source": [
    "## Part 4: Integrate Observability with LangGraph\n",
    "\n",
    "Now let's add observability to our contract analysis agent from Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62d346ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangGraph components imported\n"
     ]
    }
   ],
   "source": [
    "# Import from Notebook 1\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(Path.cwd().parent / \".env\", override=True)\n",
    "\n",
    "# Import LangGraph components\n",
    "import fitz\n",
    "from typing import TypedDict, List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph, END\n",
    "import uuid\n",
    "\n",
    "print(\"‚úÖ LangGraph components imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb55266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ContractAnalysisState imported from src/agent/state.py\n"
     ]
    }
   ],
   "source": [
    "# Import state definition from src/agent/state.py\n",
    "from agent.state import ContractAnalysisState, create_initial_state\n",
    "\n",
    "print(\"‚úÖ ContractAnalysisState imported from src/agent/state.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ec033",
   "metadata": {},
   "source": [
    "### Observed Classification Node\n",
    "\n",
    "Classification with full observability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f40ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Observed classification node defined\n"
     ]
    }
   ],
   "source": [
    "class ContractClassification(BaseModel):\n",
    "    contract_type: str\n",
    "    complexity: str\n",
    "    confidence_score: float\n",
    "    reasoning: str\n",
    "\n",
    "def classify_contract_observed(state: ContractAnalysisState) -> ContractAnalysisState:\n",
    "    \"\"\"\n",
    "    Classify contract with full observability instrumentation.\n",
    "    \"\"\"\n",
    "    request_id = state['request_id']\n",
    "    \n",
    "    # Set logging context\n",
    "    logger.set_request_context(\n",
    "        request_id=request_id,\n",
    "        user_id=state['user_id']\n",
    "    )\n",
    "    \n",
    "    # Start trace span\n",
    "    with tracer.trace_span(\n",
    "        \"classify_contract\",\n",
    "        attributes={\n",
    "            \"request.id\": request_id,\n",
    "            \"user.id\": state['user_id'],\n",
    "            \"text.length\": len(state['contract_text'])\n",
    "        }\n",
    "    ) as span:\n",
    "        try:\n",
    "            logger.info(\"Starting contract classification\")\n",
    "            \n",
    "            # Initialize LLM\n",
    "            llm = ChatOpenAI(\n",
    "                model=os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\"),\n",
    "                temperature=0\n",
    "            )\n",
    "            structured_llm = llm.with_structured_output(ContractClassification)\n",
    "            \n",
    "            prompt = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"\"\"You are an expert contract analyst. Classify the contract.\n",
    "                \n",
    "Contract Types: NDA, SaaS, Employment, Partnership, Unknown\n",
    "Complexity: Simple, Moderate, Complex\"\"\"),\n",
    "                (\"user\", \"Classify this contract:\\n\\n{contract_text}\")\n",
    "            ])\n",
    "            \n",
    "            chain = prompt | structured_llm\n",
    "            text_sample = state['contract_text'][:4000]\n",
    "            \n",
    "            # Measure LLM call\n",
    "            with tracer.trace_span(\n",
    "                \"llm_call_classify\",\n",
    "                attributes={\"model\": \"gpt-4o-mini\", \"max_tokens\": 500}\n",
    "            ):\n",
    "                result = chain.invoke({\"contract_text\": text_sample})\n",
    "            \n",
    "            # Record metrics\n",
    "            metrics.record_request(\n",
    "                contract_type=result.contract_type,\n",
    "                status=\"success\"\n",
    "            )\n",
    "            metrics.record_llm_tokens(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                operation=\"classify\",\n",
    "                tokens=400  # Approximate\n",
    "            )\n",
    "            \n",
    "            # Update state\n",
    "            state['contract_type'] = result.contract_type\n",
    "            state['complexity'] = result.complexity\n",
    "            state['confidence_score'] = result.confidence_score\n",
    "            \n",
    "            # Log success\n",
    "            logger.info(\"Classification successful\", extra={\n",
    "                \"contract_type\": result.contract_type,\n",
    "                \"complexity\": result.complexity,\n",
    "                \"confidence\": result.confidence_score\n",
    "            })\n",
    "            \n",
    "            # Add trace event\n",
    "            span.add_event(\"classification_complete\", {\n",
    "                \"type\": result.contract_type,\n",
    "                \"confidence\": result.confidence_score\n",
    "            })\n",
    "            \n",
    "            print(f\"  ‚úÖ Classified as {result.contract_type} ({result.confidence_score:.2f})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Record error metrics\n",
    "            metrics.record_request(contract_type=\"Unknown\", status=\"error\")\n",
    "            \n",
    "            # Log error\n",
    "            logger.error(\"Classification failed\", extra={\n",
    "                \"error\": str(e),\n",
    "                \"error_type\": type(e).__name__\n",
    "            })\n",
    "            \n",
    "            # Record in span\n",
    "            span.record_exception(e)\n",
    "            span.set_status(\"error\", str(e))\n",
    "            \n",
    "            state['errors'].append(f\"Classification error: {str(e)}\")\n",
    "            state['contract_type'] = \"Unknown\"\n",
    "            \n",
    "        finally:\n",
    "            logger.clear_request_context()\n",
    "    \n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Observed classification node defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb5eb75",
   "metadata": {},
   "source": [
    "### Test Observed Classification\n",
    "\n",
    "Let's test with a sample contract:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9fdda30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running observed classification...\n",
      "\n",
      "2025-11-29 11:20:05,596 - notebook_demo - INFO - Starting contract classification\n",
      "2025-11-29 11:20:08,549 - notebook_demo - INFO - Classification successful\n",
      "  ‚úÖ Classified as NDA (95.00)\n",
      "\n",
      "Results:\n",
      "   Type: NDA\n",
      "   Complexity: Simple\n",
      "   Confidence: 9500.00%\n",
      "\n",
      "Logs, metrics, and traces recorded!\n"
     ]
    }
   ],
   "source": [
    "# Sample contract text\n",
    "sample_nda_text = \"\"\"\n",
    "NON-DISCLOSURE AGREEMENT\n",
    "\n",
    "This Non-Disclosure Agreement (\"Agreement\") is entered into as of January 1, 2024,\n",
    "by and between TechCorp Inc. (\"Disclosing Party\") and John Doe (\"Receiving Party\").\n",
    "\n",
    "1. CONFIDENTIAL INFORMATION\n",
    "For purposes of this Agreement, \"Confidential Information\" means all information\n",
    "disclosed by Disclosing Party to Receiving Party.\n",
    "\n",
    "2. OBLIGATIONS\n",
    "Receiving Party agrees to:\n",
    "a) Maintain confidentiality of all Confidential Information\n",
    "b) Not disclose to any third parties\n",
    "c) Use only for the Purpose described herein\n",
    "\n",
    "3. TERM\n",
    "This Agreement shall remain in effect for 2 years from the Effective Date.\n",
    "\"\"\"\n",
    "\n",
    "# Create test state using the helper function from src/agent/state.py\n",
    "test_state = create_initial_state(\n",
    "    contract_text=sample_nda_text,\n",
    "    file_path=\"sample_nda.txt\",\n",
    "    user_id=\"demo_user\"\n",
    ")\n",
    "\n",
    "print(\"Running observed classification...\\n\")\n",
    "result = classify_contract_observed(test_state)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"   Type: {result['contract_type']}\")\n",
    "print(f\"   Complexity: {result['complexity']}\")\n",
    "print(f\"   Confidence: {result['confidence_score']:.2%}\")\n",
    "print(f\"\\nLogs, metrics, and traces recorded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eacb31",
   "metadata": {},
   "source": [
    "## Part 5: Metrics Dashboard (Simulated)\n",
    "\n",
    "In production, Grafana would visualize these metrics. Let's simulate a dashboard view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd9b115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Simulating 100 contract analyses...\n",
      "\n",
      "================================================================================\n",
      "                    CONTRACT ANALYSIS DASHBOARD\n",
      "================================================================================\n",
      "\n",
      "üìà REQUEST METRICS\n",
      "   Total Requests:     100\n",
      "   Success Rate:       92%\n",
      "   Error Rate:         8%\n",
      "\n",
      "üìä CONTRACT TYPE BREAKDOWN\n",
      "   Partnership      26 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   SaaS             24 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   Employment       23 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "   NDA              19 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\n",
      "‚è±Ô∏è  PERFORMANCE METRICS\n",
      "   Avg Duration:       2.45s\n",
      "   Min Duration:       1.04s\n",
      "   Max Duration:       3.94s\n",
      "   P95 Duration:       3.70s\n",
      "\n",
      "ü™ô TOKEN USAGE\n",
      "   Total Tokens:       126,152\n",
      "   Avg Tokens/Request: 1,371\n",
      "   Est. Cost (GPT-4o-mini): $0.02\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚ÑπÔ∏è  In production, this data appears in Grafana dashboards with:\n",
      "   ‚Ä¢ Time-series graphs\n",
      "   ‚Ä¢ Real-time alerts\n",
      "   ‚Ä¢ Custom panels\n",
      "   ‚Ä¢ SLA tracking\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Simulate 100 contract analyses\n",
    "print(\"üìä Simulating 100 contract analyses...\\n\")\n",
    "\n",
    "contract_types = [\"NDA\", \"SaaS\", \"Employment\", \"Partnership\"]\n",
    "stats = defaultdict(int)\n",
    "total_tokens = 0\n",
    "durations = []\n",
    "\n",
    "for i in range(100):\n",
    "    contract_type = random.choice(contract_types)\n",
    "    success = random.random() > 0.05  # 95% success rate\n",
    "    \n",
    "    # Record metrics\n",
    "    metrics.record_request(\n",
    "        contract_type=contract_type,\n",
    "        status=\"success\" if success else \"error\"\n",
    "    )\n",
    "    \n",
    "    if success:\n",
    "        tokens = random.randint(800, 2000)\n",
    "        duration = random.uniform(1.0, 4.0)\n",
    "        \n",
    "        metrics.record_llm_tokens(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            operation=\"analyze\",\n",
    "            tokens=tokens\n",
    "        )\n",
    "        metrics.record_duration(\n",
    "            operation=\"analyze\",\n",
    "            duration=duration\n",
    "        )\n",
    "        \n",
    "        stats[contract_type] += 1\n",
    "        total_tokens += tokens\n",
    "        durations.append(duration)\n",
    "\n",
    "# Display dashboard-style summary\n",
    "print(\"=\" * 80)\n",
    "print(\"                    CONTRACT ANALYSIS DASHBOARD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìà REQUEST METRICS\")\n",
    "print(f\"   Total Requests:     100\")\n",
    "print(f\"   Success Rate:       {len(durations)}%\")\n",
    "print(f\"   Error Rate:         {100-len(durations)}%\")\n",
    "\n",
    "print(\"\\nüìä CONTRACT TYPE BREAKDOWN\")\n",
    "for ctype, count in sorted(stats.items(), key=lambda x: x[1], reverse=True):\n",
    "    bar = \"‚ñà\" * (count // 2)\n",
    "    print(f\"   {ctype:15} {count:3} {bar}\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  PERFORMANCE METRICS\")\n",
    "print(f\"   Avg Duration:       {sum(durations)/len(durations):.2f}s\")\n",
    "print(f\"   Min Duration:       {min(durations):.2f}s\")\n",
    "print(f\"   Max Duration:       {max(durations):.2f}s\")\n",
    "print(f\"   P95 Duration:       {sorted(durations)[int(len(durations)*0.95)]:.2f}s\")\n",
    "\n",
    "print(\"\\nü™ô TOKEN USAGE\")\n",
    "print(f\"   Total Tokens:       {total_tokens:,}\")\n",
    "print(f\"   Avg Tokens/Request: {total_tokens//len(durations):,}\")\n",
    "print(f\"   Est. Cost (GPT-4o-mini): ${total_tokens * 0.00015 / 1000:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n‚ÑπÔ∏è  In production, this data appears in Grafana dashboards with:\")\n",
    "print(\"   ‚Ä¢ Time-series graphs\")\n",
    "print(\"   ‚Ä¢ Real-time alerts\")\n",
    "print(\"   ‚Ä¢ Custom panels\")\n",
    "print(\"   ‚Ä¢ SLA tracking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1717162a",
   "metadata": {},
   "source": [
    "## Part 6: Starting the Observability Stack\n",
    "\n",
    "To view metrics and traces in production tools, start the Docker stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c79127f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê≥ To start the full observability stack:\n",
      "\n",
      "1. Open terminal and navigate to project root:\n",
      "   cd c:\\000 - Zensar - AI For Leaders\\References\\zensar-day6\\enterprise-contract-agent\n",
      "\n",
      "2. Start Docker services:\n",
      "   docker-compose up -d\n",
      "\n",
      "3. Access the dashboards:\n",
      "   ‚Ä¢ Prometheus:  http://localhost:9090\n",
      "   ‚Ä¢ Grafana:     http://localhost:3000 (admin/admin)\n",
      "   ‚Ä¢ Jaeger:      http://localhost:16686 (traces)\n",
      "\n",
      "4. In Grafana:\n",
      "   - Add Prometheus datasource (http://prometheus:9090)\n",
      "   - Import dashboard from dashboards/contract_agent.json\n",
      "\n",
      "5. Run the agent and see metrics appear in real-time!\n",
      "\n",
      "\n",
      "‚úÖ Instructions displayed above\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"üê≥ To start the full observability stack:\n",
    "\n",
    "1. Open terminal and navigate to project root:\n",
    "   cd c:\\\\000 - Zensar - AI For Leaders\\\\References\\\\zensar-day6\\\\enterprise-contract-agent\n",
    "\n",
    "2. Start Docker services:\n",
    "   docker-compose up -d\n",
    "\n",
    "3. Access the dashboards:\n",
    "   ‚Ä¢ Prometheus:  http://localhost:9090\n",
    "   ‚Ä¢ Grafana:     http://localhost:3000 (admin/admin)\n",
    "   ‚Ä¢ Jaeger:      http://localhost:16686 (traces)\n",
    "\n",
    "4. In Grafana:\n",
    "   - Add Prometheus datasource (http://prometheus:9090)\n",
    "   - Import dashboard from dashboards/contract_agent.json\n",
    "\n",
    "5. Run the agent and see metrics appear in real-time!\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Instructions displayed above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90146151",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### **What We Learned:**\n",
    "\n",
    "1. **Structured Logging**\n",
    "   - JSON format for machine parsing\n",
    "   - Context management for request tracking\n",
    "   - Rich metadata for debugging\n",
    "\n",
    "2. **Prometheus Metrics**\n",
    "   - Counter, Gauge, Histogram types\n",
    "   - Custom business metrics (contract types, tokens)\n",
    "   - Performance tracking (duration, throughput)\n",
    "\n",
    "3. **OpenTelemetry Tracing**\n",
    "   - Distributed traces across services\n",
    "   - Span hierarchy for nested operations\n",
    "   - Custom attributes and events\n",
    "\n",
    "4. **Integration Patterns**\n",
    "   - Instrumenting LangGraph nodes\n",
    "   - Error handling with observability\n",
    "   - Context propagation\n",
    "\n",
    "### **Production Benefits:**\n",
    "\n",
    "- **Faster debugging** - Find issues in minutes, not hours\n",
    "- **Cost optimization** - Track token usage per user/contract type\n",
    "- **SLA compliance** - Monitor and alert on performance\n",
    "- **Capacity planning** - Predict scaling needs\n",
    "- **User experience** - Identify slow operations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
